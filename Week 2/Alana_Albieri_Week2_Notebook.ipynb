{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oUCqgWH79ByF"
      },
      "source": [
        "# Data Engineering in Energy\n",
        "### Albieri M. Alana, Estudiante de Ing Informática URBE\n",
        "### 2026\n",
        "***\n",
        "## Week 2 Workbook\n",
        "***"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##**1.\tDefine and describe the following:**"
      ],
      "metadata": {
        "id": "RgcM-omH9_yi"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "###**a.\tFeature Imputation**\n",
        "\n",
        "Al hablar de Feature Imputation hacemos referencia a que hacer cuando tenemos valores nulos o faltantes en nuestro dataset, tenemos por ejemplo la opcion de tomar valores de tendencia central como la media para rellenarles, sin embargo, debemos tomar en cuenta que al tener outliers es posible que esos datos rellenados tengan sesgos. Otra opcion es predecirles usando regresiones lineales, bajo costo computacional sin embargo es muy sensible ya que es valga la redundancia lineal, si se quiere mayor exactitud esta el algoritmo knn que toma el promedio de los vecinos mas cercanos. Cada algoritmo tiene sus pros y contras, su implementacion dependera de las necesidades del negocio."
      ],
      "metadata": {
        "id": "YqGEcX4lJbei"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "###**b.\tFeature Transformation**\n",
        "\n",
        "El Feature Transformation esta enfocado a la transformacion de los datos segun los requerimientos y escalas del mismo, como ejemplo tengamos una correlacion entre edad y salarios, dos tipos de enfermedades en grados o similar, basicamente comparando dos variables con cantidades medibles muy diferentes llevarles a un mismo nivel. Una de las estrategias mas famosas es la de normalizacion de min-max que consiste en llevarle de la cantidad inicial a su respectivo entre 0-1."
      ],
      "metadata": {
        "id": "uVEsGAEXQy3R"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "###**c.\tFeature Ranking**\n",
        "\n",
        "Basicamente se centra en asignar prioridades en nuestras carecteristicas para nuestro modelo, determinando cual tiene mayor importancia en comparacion a el resto, creando un \"Ranking\" de importancias, de la mayora a menor."
      ],
      "metadata": {
        "id": "aX4qZCAmquPw"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "###**d.\tEpistemic vs. Aleatoric Uncertainty**\n",
        "\n",
        "Consiste en dos formas distintas de clasificar la incertidumbre, la incertidumbre epistemica tiene como base el desconocimiento. Es decir, se basa en que desconoce un hecho para calcular la incertidumbre sin emabrgo estos niveles de incertidumbre pueden bajar a medida que conozcamos mas del hecho a diferencia de la incertidumbre aleatoria que como bien dice el nombre es completamente aleatoria y fija, la misma no depende de nosotros para bajar o aumentar."
      ],
      "metadata": {
        "id": "-VZiiDXbsETc"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "###**e.\tCurse of Dimensionality**\n",
        "\n",
        "Podemos pensar que mientras mas variables tenemos en nuestro modelo, es mejor, ¿o no?. No, por el contrario debemos calcular la cantidad viable de variables para que la misma no se vea afectada al tener pocos datos y una cantidad excesiva de varibles. Es ese concepto reside la \"Curse of Dimensionality\", muchas variables y pocos datos."
      ],
      "metadata": {
        "id": "Ja488jWfxvsB"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "###**f.\tCausation vs. Correlation**\n",
        "\n",
        "Una fuerte correlacion conlleva siempre una causalidad fija cierto? No, al hablar de correlacion tomamos en cuenta que dos variables se mueven a un ritmo similar, sin embargo no siempre el hecho de A implica B. Supongamos que tenemos una fuerte correlacion entre paraguas y accidentes de vehiculos, casi de 1, eso quiere decir que los paraguas son causantes de los accidentes? no, tenemos una variable oculta en este caso la lluvia que seria la variable C quien conecta e influye en las otras 2. Para validar que dos variables con fuerte correlacion tengan causas similares se suele hacer testing A/B."
      ],
      "metadata": {
        "id": "me9euFdW1gOy"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "***\n",
        "##**2.\tFeature Imputation**"
      ],
      "metadata": {
        "id": "suKdtTF1-Q6o"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "###**a.\tLoad the dataset as a pandas dataframe and provide a summary of the features using “pd.describe()”. How many NaN’s are there and what percent of values in the dataframe are NaNs? Pay special attention to the minimum/maximum and mean/stdv values of each feature.**"
      ],
      "metadata": {
        "id": "Zg3cRQLQ-hkN"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "bwRLI4wk-k5f"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IgOb_YAo9ByH"
      },
      "source": [
        "***\n",
        "# END"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "nemo",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.11"
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}